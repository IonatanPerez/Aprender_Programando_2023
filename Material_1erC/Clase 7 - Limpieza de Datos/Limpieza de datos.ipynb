{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accomplished-modeling",
   "metadata": {
    "id": "federal-actress"
   },
   "source": [
    "# Limpieza de datos\n",
    " \n",
    "La limpieza de datos es uno de los pasos más importantes en un proyecto de análisis de datos ya que si los datos están \"sucios\", el análisis también lo será.\n",
    " \n",
    "Para evaluar la validez de los datos de nuestro dataset, es importante analizar y corroborar si los datos cumplen o se ajustan a la reglas o restricciones propias del dato:\n",
    "* **De tipo de dato**: los valores en una columna en particular deben ser de un tipo de datos.\n",
    "* **De rango**: generalmente, los números o fechas deben estar dentro de un cierto rango.\n",
    "* **Obligatorias**: determinadas columnas no pueden estar vacías.\n",
    "* **Únicas**: un campo, debe ser único en un conjunto de datos.\n",
    "* **De pertenencia al conjunto**: los valores de una columna provienen de un conjunto de valores discretos. Por ejemplo, el sexo biológico de una persona en general se marca como masculino o femenino.\n",
    "* **Patrones de expresión regular**: campos de texto que deben seguir un patrón determinado. (Email)\n",
    "* **Validación de campo cruzado**: deben cumplirse determinadas condiciones que abarcan varios campos. Por ejemplo, la fecha de alta de un paciente del hospital no puede ser anterior a la fecha de admisión.\n",
    " \n",
    "\n",
    "Vamos a hacer una revisión de las columnas para ver si hay datos erroneos. También vamos a revisar si hay datos nulos (registros vacíos). Vamos a resolver estos problemas utilizando métodos de la librería Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-formula",
   "metadata": {
    "id": "outstanding-burlington"
   },
   "outputs": [],
   "source": [
    "#importamos las librerias que utilizaremos\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-anderson",
   "metadata": {
    "id": "smaller-democracy"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://cdn.buenosaires.gob.ar/datosabiertos/datasets/arbolado-publico-lineal/arbolado-publico-lineal-2017-2018.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-advocate",
   "metadata": {},
   "source": [
    "### Errores en la carga de los datos\n",
    "\n",
    "En muy comun al utilizar datos de la vida real que haya errores en la carga y es necesario realizar una limpieza para normalizar los datos y asegurarse que todo este cargado de la misma manera. Es necesario realizar esto previo a hacer visualizaciones para poder hacer un análisis de datos de mejor calidad.\n",
    "\n",
    "En este caso análizaremos las columnas *estado_plantera*, *ubicación_planera* y *nivel_planteara* para realizar una limpieza con distintas técnicas de la librearía Pandas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-diameter",
   "metadata": {},
   "source": [
    "#### Columna *ubicación_plantera*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"ubicacion_plantera\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-discretion",
   "metadata": {},
   "source": [
    "En este caso vemos varios problemas: datos que son leídos como diferentes pero en realidad son iguales por diferencia entre mínisculas y mayúsculas o por espacios de más (ejemplo: \"Regular \"), también se ven campos que están mal escritos como \"Ochava\" con \"Ochva\" y \"Och\" y datos que hay solamente una \"o\" que puede ser \"Ochava\" u \"Ocupada\". Como no lo sabemos en este caso vamos a dejar el dato nulo.\n",
    "\n",
    "Para poner dato nulo vamos a utilizar un método de la librería Numpy = np.NaN (Not a Number)\n",
    "\n",
    "Vamos a usar el método método [replace](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace(\"Regular \", \"Regular\", inplace=True)\n",
    "data.replace(\"regular\", \"Regular\",inplace=True)\n",
    "data.replace(\"o\" , np.NaN,inplace=True)\n",
    "data.replace(\"O\", np.NaN,inplace=True)\n",
    "data.replace(\"Och\", \"Ochava\",inplace=True)\n",
    "data.replace(\"Ochva\", \"Ochava\",inplace=True)\n",
    "data.replace(\"ochava\" , \"Ochava\",inplace=True)\n",
    "data.replace(\"Fuera Línea,Ochava\", \"Ochava/Furea Linea\",inplace=True)\n",
    "data.replace(\"Fuera de Línea, Ochava\", \"Ochava/Fuera Linea\",inplace=True)\n",
    "data.replace(\"Ochava/Fuera Línea\", \"Ochava/Fuera Linea\",inplace=True)\n",
    "data.replace(\"Fuera Línea/Ochava\", \"Ochava/Fuera Linea\",inplace=True)\n",
    "data.replace(\"Ochava/Furea Linea\", \"Ochava/Fuera Linea\",inplace=True)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-consumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"ubicacion_plantera\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"ubicacion_plantera\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-looking",
   "metadata": {},
   "source": [
    "#### Columna *nivel_plantera*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-banks",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"nivel_plantera\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"nivel_plantera\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-utilization",
   "metadata": {},
   "source": [
    "También notamos que varios valores están escrito de distintas maneras, vamos a utilizar nuevamente el método [replace](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html) pero en este caso vamos a crear un diccionario para hacer el reemplazo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-coupon",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc_reemplazar= {\"Bajo nivel\":\"Bajo Nivel\",\n",
    "                  \"EL\": \"Elevadas\",\n",
    "                  \"BN\": \"Bajo Nivel\",\n",
    "                  \"Bajo Bivel\" : \"Bajo Nivel\",\n",
    "                  \"Elevado\" : \"Elevadas\",\n",
    "                  \"Eleveda\": \"Elevadas\",\n",
    "                  \"elevada\":\"Elevadas\",\n",
    "                  \"Elevada\":\"Elevadas\",\n",
    "                  \"Baja Nivel\": \"Bajo Nivel\",\n",
    "                  \"ELEVADA\": \"Elevadas\",\n",
    "                  \"el\":\"Elevadas\",\n",
    "                  \"bajo nivel\": \"Bajo Nivel\",\n",
    "                  \"Bajo  nivel\": \"Bajo Nivel\",\n",
    "                   \"obs: no tiene plantera definida\": np.NaN\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-productivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace(dicc_reemplazar, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"nivel_plantera\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# También se puede colocar dentro del reemplace una lista dentro de un diccionario\n",
    "\n",
    "\n",
    "data.replace([\"A nivel\",\"AN\",  \"A  Nivel\",\"a Nivel\", \"A nivel \", \"A Nivel\", \"an\", \"An\"], \"A Nivel\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"nivel_plantera\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-check",
   "metadata": {},
   "source": [
    "#### Columna  *estado_plantera*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"estado_plantera\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-broad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"estado_plantera\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-pharmaceutical",
   "metadata": {},
   "source": [
    "Podemos analizar que \"Sobreocupada\", \"sobreocupada\" y \"SobreOcupada\" es el mismo valor, solo que está escrito de distintas maneras ya que python (y la mayoría de los lenguajes) es sensible a las mayusculas y minusculas. Para resolver este tema vamos a utilizar el método [.Lower](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.lower.html) de Python y dejar todos los campos en minúsucula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-accreditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"estado_plantera\"] = data[\"estado_plantera\"].str.lower()\n",
    "\n",
    "# Observar que en este caso reemplazamos la columna, podrìamos crear una nueva con otro nombre y luego eliminar la anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"estado_plantera\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-shirt",
   "metadata": {},
   "source": [
    "Si observamos con antención hay un espacio en la segunda \"ocupada\", por eso esta identificada como distinta, en este caso vamos a utilizar dos métodos de pandas: *apply* y *lambda*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-vector",
   "metadata": {},
   "source": [
    "- [apply](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.apply.html): puede realizarse sobre un DataFrame y opera sobre filas o columnas completas o sobre una Series (una sola columna) y opera sobre cada uno de los elementos.\n",
    "\n",
    "- **funciones lambda** también se denominan funciones anónimas. Una función anónima es una función definida sin un nombre. Como sabemos, para definir una función normal en Python, usamos la palabra clave *def*, pero en el caso de funciones anónimas, usamos la palabra clave *lambda* para definirlas.\n",
    "\n",
    "La sintaxis es \n",
    "\n",
    "    lambda argumento: expresión\n",
    "\n",
    "Dentro de la expresión se realizará la función que se desea realizar.\n",
    "\n",
    "En este caso definiremos la función lambda para realizar un condicional donde reemplace los valores \"ocuapada \" por un \"ocupada\" y sino que deje el valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define una nueva columna \"estado_plantera\" con \"apply\" se corre la función \"lambda\" por cada uno de los valores de las celdas de nuestra Series\n",
    "# x es el argumento que en este caso se refiere a cada valor de la columna\n",
    "\n",
    "data[\"estado_plantera\"] = data[\"estado_plantera\"].apply(lambda x: \"ocupada\" if x==\"ocupada \" else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-photography",
   "metadata": {},
   "source": [
    "En el mismo sentido podemos considerar que \"cantero ocupado\" y \"ocupada\" son lo mismo, está es una decisión del/la analista de datos teniendo conociemiento sobre el tema que estamos trabajando. \n",
    "\n",
    "Vamos a utilizar nuevamente el método [replace](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace(\"cantero ocupado\", \"ocupada\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"estado_plantera\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-blind",
   "metadata": {
    "id": "entitled-house"
   },
   "source": [
    "#### Identificación de datos nulos\n",
    "\n",
    "Ahora vamos a limpiar los datos nulos que encontramos en nuestro dataset. \n",
    "Para observar si hay datos nulos se puede utilizar el método *isnull()* o *isna()* el cual devovlerá **True** en el caso que haya un dato nulo y **False** en el caso que no lo haya.\n",
    "Esto puede ser utilizado como una máscara para filtrar el dataset y ver los registros nulos.\n",
    "\n",
    "Para saber la cantidad de datos nulos es posible utilizar *sum()* luego de la identificación de los nulos sabiendo que **True** cuenta como 1 y **False** cuenta como 0, de esta manera sumara 1 por cada dato nulo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-velvet",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1646445777857,
     "user": {
      "displayName": "Valeria Bellino",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05918551666787590515"
     },
     "user_tz": 180
    },
    "id": "inclusive-binary",
    "outputId": "c54891ff-bd58-415b-f316-2d800529775d"
   },
   "outputs": [],
   "source": [
    "data.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-diagnosis",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1646445779994,
     "user": {
      "displayName": "Valeria Bellino",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05918551666787590515"
     },
     "user_tz": 180
    },
    "id": "voluntary-lover",
    "outputId": "65853689-ec14-4623-fd64-42034028cb61"
   },
   "outputs": [],
   "source": [
    "data.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-sword",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1646446681630,
     "user": {
      "displayName": "Valeria Bellino",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05918551666787590515"
     },
     "user_tz": 180
    },
    "id": "expensive-invitation",
    "outputId": "1cccd74c-32a6-494d-9b9f-8234e3b0b5bc"
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-albania",
   "metadata": {
    "id": "latter-hardwood"
   },
   "source": [
    "Podemos observar que hay varias variables quue tienen datos nulos, frente a eso se pueden realizar distintas estrategias: **imputar**, **marcar** o **eliminar**, eso dependerá de la importancia de la columna, de la cantida de datos nulos y es decisión del/la Analista de datos. \n",
    "\n",
    "#### Columna *manzana*\n",
    "\n",
    "En este caso vamos a analizar qué columnas no son importantes para nuestro análisis por ejemplo la columna *manzana*, se interpreta que tiene muchos nulos porque no en todas las direcciones se indica este dato, pero vemos que de 370.180, 146.040 son nulos, es decir casi el 40% es nulo.\n",
    "\n",
    "En este caso vamos a eliminar la columna con el método [drop](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=\"manzana\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-reward",
   "metadata": {},
   "source": [
    "#### Columna *calle_nombre*\n",
    "\n",
    "Vamos a ver los registros nulos de de *calle_nombre*, para eso vamos a hacer una máscara y filtrar el dataset para verlo mejor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-halifax",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_nulo_calle = data[\"calle_nombre\"].isnull()\n",
    "mask_nulo_calle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-method",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1646446816777,
     "user": {
      "displayName": "Valeria Bellino",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05918551666787590515"
     },
     "user_tz": 180
    },
    "id": "lyric-disposal",
    "outputId": "a7b65827-b4e2-4a20-b00e-645846d27ef9"
   },
   "outputs": [],
   "source": [
    "data.loc[mask_nulo_calle]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-belfast",
   "metadata": {
    "id": "effective-decision"
   },
   "source": [
    "En este caso observamos que los registros que tienen nulo el *calle_nombre* tienen varios datos faltantes de muchas columnas, no tenemos ningún dato de ubicación por lo que en este caso tal vez vamos a eliminar los registros que tienen este dato nulo\n",
    "\n",
    "En este caso lo que realizaremos es **eliminar** las filas que tienen muchos datos nulos utilizando el método [dropna](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html). Este método permite eliminar filas o columnas y se pueden definir distintos criterios. En este caso eliminaremos los registos que tienen nulo esta columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-assumption",
   "metadata": {
    "id": "outside-interest"
   },
   "outputs": [],
   "source": [
    "data.dropna(subset=[\"calle_nombre\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-humor",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1646446747948,
     "user": {
      "displayName": "Valeria Bellino",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05918551666787590515"
     },
     "user_tz": 180
    },
    "id": "concerned-medicare",
    "outputId": "82ebbd0f-85e0-4025-82d4-cf5ddc5c017f"
   },
   "outputs": [],
   "source": [
    "# Revisamos la operación\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-tract",
   "metadata": {},
   "source": [
    "#### Columna *nivel_plantera*\n",
    "\n",
    "En este caso vemos que hay 1324 nulos que representan menos del 1% de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-blocking",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[\"nivel_plantera\"].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-referral",
   "metadata": {
    "id": "acute-colleague"
   },
   "source": [
    "Podemos observar que es solamente uno o dos datos y no se observa ninguna otra particularidad ya que el resto de los valores parecen correctos.\n",
    "\n",
    "En este caso lo que realizaremos es **imputar** de manera aleatoria el dato faltante. Al ser una variable categórica utilizaremos la **moda**, es decir, el valor que más veces se repite. Para esto volveremos a observar como estás distribuidos los valores que puede tomar la columna y luego definiremos con *.loc* el dato faltante y lo completaremos.\n",
    "\n",
    "En este caso lo realizaremos de manera aleatoria utilizando como criterio la **moda**, también podría imputarse los datos faltantes en base a los otros datos de las columnas que si tenemos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"nivel_plantera\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-exclusion",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "executionInfo": {
     "elapsed": 330,
     "status": "error",
     "timestamp": 1646621949172,
     "user": {
      "displayName": "Valeria Bellino",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05918551666787590515"
     },
     "user_tz": 180
    },
    "id": "ambient-acting",
    "outputId": "7499870f-7a34-4ca5-c43f-0f40ee1adebc"
   },
   "outputs": [],
   "source": [
    "# Como la moda es \"A Nivel\" (el valor que más se repite) imputaremos con ese valor (que además corresponde con la humedad)\n",
    "\n",
    "# Seleccionaremos el dato que queremos imputar\n",
    "\n",
    "data.loc[data[\"nivel_plantera\"].isnull()] = \"A Nivel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-mailing",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1646446850129,
     "user": {
      "displayName": "Valeria Bellino",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05918551666787590515"
     },
     "user_tz": 180
    },
    "id": "liquid-ceramic",
    "outputId": "434b9bcd-dadd-4ef0-b900-94154eef6f76"
   },
   "outputs": [],
   "source": [
    "# Revisamos la operación\n",
    "\n",
    "data.loc[data[\"nivel_plantera\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-corporation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1646446852840,
     "user": {
      "displayName": "Valeria Bellino",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05918551666787590515"
     },
     "user_tz": 180
    },
    "id": "associate-universe",
    "outputId": "900cc6cb-8814-46a8-89c8-4bbc83a1fec5"
   },
   "outputs": [],
   "source": [
    "# Comprobamos que este correctamente imputado y que no haya más datos nulos en lluvia\n",
    "\n",
    "print(data[\"nivel_plantera\"].value_counts())\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-american",
   "metadata": {
    "id": "greek-jersey"
   },
   "source": [
    "#### Columna *ubicacion_plantera*\n",
    "\n",
    "Ahora realizaremos el mismo procedimiento en el caso de los datos nulos en la variable *ubicacion_plantera*, utilizaremos *isnull()* para crear una máscara y luego filtraremos con *.loc.* para observar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-nerve",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1646446869379,
     "user": {
      "displayName": "Valeria Bellino",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05918551666787590515"
     },
     "user_tz": 180
    },
    "id": "liable-bennett",
    "outputId": "0c56fe7d-1268-4460-d83d-9c4bcee2039a"
   },
   "outputs": [],
   "source": [
    "data.loc[data[\"ubicacion_plantera\"].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-soldier",
   "metadata": {
    "id": "italian-cooling"
   },
   "source": [
    "En este caso nuevamente lo que realizaremos es **imputar** de manera aleatoria el dato faltante. Para esto volveremos a observar los datos que puede tomar esta columna y luego definiremos con *.loc* el dato faltante y lo completaremos. Esta imputación será creando una lista aleatoria de valores para completar los datos faltantes. \n",
    "\n",
    "Para crear esta lista utilizaremos la libreria *random* cuyo método *choices* permite la generación de una lista aleatoria de los 4 valores que más se repiten. Para ver más de esta librería y practicar puede consultaste [w3schools](https://www.w3schools.com/python/ref_random_choice.asp)\n",
    "\n",
    "Aquí también lo realizaremos de manera aleatoria, pero podría imputarse los datos faltantes en base a los otros datos de las columnas que si tenemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-insulation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1646446887241,
     "user": {
      "displayName": "Valeria Bellino",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05918551666787590515"
     },
     "user_tz": 180
    },
    "id": "compliant-spyware",
    "outputId": "d04632a9-2258-4b46-8ed5-edb0206b3dd8"
   },
   "outputs": [],
   "source": [
    "# Primero confirmaremos los valores que puede tomar la columna \"descripcion\"\n",
    "\n",
    "data[\"ubicacion_plantera\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-retrieval",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1646446889681,
     "user": {
      "displayName": "Valeria Bellino",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05918551666787590515"
     },
     "user_tz": 180
    },
    "id": "raising-member",
    "outputId": "edc926fe-3adb-459c-d9b9-5ae03c5ea28c"
   },
   "outputs": [],
   "source": [
    "# Creamos la lista que tome los valores que indiquemos y k es la cantidad de valores que se generaran, en este caso 629\n",
    "\n",
    "import random\n",
    "\n",
    "lista_imputar = random.choices([\"Regular\", \"Ochava\", \"Fuera de linea\", \"A Nivel\"], k=629)  #correr varias veces para ver como se modifica\n",
    "lista_imputar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-awareness",
   "metadata": {
    "id": "cosmetic-robertson"
   },
   "outputs": [],
   "source": [
    "# Volvemos a seleccionar los campos que queremos imputar y definimos los campos con la lista creada en el paso anterior\n",
    "\n",
    "data.loc[data[\"ubicacion_plantera\"].isnull(), \"ubicacion_plantera\"]=lista_imputar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-clarity",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1646446898158,
     "user": {
      "displayName": "Valeria Bellino",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05918551666787590515"
     },
     "user_tz": 180
    },
    "id": "exact-portsmouth",
    "outputId": "b2e1f795-3241-4588-faa7-20b1d3c4c9f2"
   },
   "outputs": [],
   "source": [
    "# Comprobamos que este correctamente imputado y que no haya más datos nulos en lluvia\n",
    "\n",
    "print(data[\"ubicacion_plantera\"].value_counts())\n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-burns",
   "metadata": {
    "id": "ZsKhfo7U4575"
   },
   "source": [
    "## Conclusión\n",
    "\n",
    "En esta notebook hemos revisado disntias formas de resolver los datos erroneos faltantes y se vieron diferentes métodos útiles para realizar estas operaciones. Hay muchos métodos de Pandas que se pueden utilizar como por ejemplo [fillna](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html), en este caso solo hemos visto algunos, es importante resaltar que en todo el proceso es el/la Analista de Datos quien debe tomar decisiones sobre los pasos a seguir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-genius",
   "metadata": {},
   "source": [
    "### Desafío 6: Buscar Datasets\n",
    "\n",
    "El objetivo es ponerse en grupos y buscar dataset que sean de su interés para poder realizar un análisis. Puede ser de cualquier tema que les interese que puedan contar con Datos. Les dejamos un Documento de Google con sitios donde pueden buscar datasets, es un Documento colaborativo entre todas las comisiones de Datos asì que pueden agregar: [Dataset](https://docs.google.com/document/d/1EnimkrV_a9cu-ZrrcydZnigquQeGSpUsnqu0BA3fUFE/edit?usp=sharing!)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Limpieza_de_datos_datos_nulos.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
